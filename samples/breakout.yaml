game:
  name: 'BreakoutNoFrameskip-v4'
  num_player: 1
  # number of total actions
  actions: 4
  # if frame_stack equals to 0, then
  #   observation_shape is [channel, H, W]
  #   feature_shape is [channel, H, W]
  # otherwise,
  #   observation_shape is [frame_stack * (channel + 1), H, W]
  #   feature_shape is [(channel + 1), H, W]
  observation:
    # number of frames to stack
    # for 0, (o_1)
    # for n, (a_1, o_1, ...,  a_n, o_n)
    frame_stack: 8
    # C
    channel: 1
    # [H, W]
    spatial_shape: [96, 96]
  # if a state_spatial_shape is [H, W],
  # then the state_shape is [h_channels, H, W]
  state_spatial_shape: [6, 6]

model:
  name: 'MuZeroAtari'
  # h: representation function
  h_blocks: 2
  h_channels: 96
  # g: dynamics function
  g_blocks: 2
  r_heads: [0, 3]
  r_loss: 'cross_entropy'
  # f: prediction function
  f_blocks: 2
  f_channels: 96
  v_heads: [0, 10]
  v_loss: 'cross_entropy'

learner:
  # transform of rewards and values
  transform: 'Atari'
  # rollout timesteps K
  rollout_steps: 5
  # train the model when there are N newly generated states
  # 10 sequence * 200 states_per_sequence
  frequency: 5_000
  # how many states to store in the replay buffer
  # usually equals to (25 * frequency)
  replay_buffer_size: 125_000
  # how many times to reuse each state in the replay buffer
  replay_buffer_reuse: 1
  # whether to enable prioritized experience replay
  prioritized: True
  # how many states to learn from per batch
  batch_size: 1024
  # save a checkpoint every N steps
  checkpoint_freq: 10
  optimizer:
    learning_rate: 0.05
    momentum: 0.9
    weight_decay: 1.e-4
    nesterov: True

mcts:
  # Mcts simulation counts
  simulation_count: 50
  # pUCT exploration constant
  c_puct: 1.25
  # discount factor of the return
  discount_factor: .997
  # n-step of the return (set `0` to disable)
  nstep: 10
  # root dirichlet noise
  dirichlet:
    # Dir(alpha)
    alpha: 0.25
    # (1 - epsilon) * policy + epsilon * Dir(a);
    epsilon: 0.25

game_server:
  # drop the softmax temperature from 1 to 0 after the number of steps
  # (set `True` to always enable)
  softmax_temperature_step: True
  # send a sequence every length N (set `0` to send full game)
  sequence: 200

evaluator:
  # evaluation mode
  mode: 'best'
  # evaluate performance every N checkpoints
  frequency: 10
