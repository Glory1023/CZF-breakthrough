game:
  name: 'tic_tac_toe'
  num_player: 2
  actions: 9
  # [h_channels, H, W]
  state_shape: [3, 3, 3]
  # [C, H, W]
  observation_shape: [3, 3, 3]

model:
  # h: representation function
  h_blocks: 2
  h_channels: 3
  # g: dynamics function
  g_blocks: 2
  r_heads: 1
  # f: prediction function
  f_blocks: 2
  f_channels: 32
  v_heads: 1

learner:
  rollout_steps: 5
  # train the model when there are N newly generated states.
  frequency: 1000
  # how many states to store in the replay buffer.
  replay_buffer_size: 20000
  # how many times to reuse each state in the replay buffer.
  replay_buffer_reuse: 1
  # how many states to learn from per batch.
  batch_size: 200
  # save a checkpoint every N steps.
  checkpoint_freq: 10
  optimizer:
    learning_rate: 0.01
    momentum: 0.9
    weight_decay: 1.e-4
    nesterov: True

mcts:
  # random number seed
  seed: 1
  # GPU wait max timeout (microseconds)
  timeout_us: 1000
  # GPU max batch size
  batch_size: 200
  # Mcts simulation counts
  simulation_count: 400
  # pUCT exploration constant
  c_puct: 1.25
  # discount factor of the return
  discount_factor: 1.
  # n-step of the return
  nstep: 10
  dirichlet:
    # Dir(alpha)
    alpha: 0.03
    # (1 - epsilon) * policy + epsilon * Dir(a);
    epsilon: 0.25
