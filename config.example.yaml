game:
  name: 'tic_tac_toe'
  num_player: 2
  actions: 9
  observation_shape: [3, 3, 3]

model:
  # h: representation function
  h_blocks: 2
  h_channels: 3
  # g: dynamics function
  g_blocks: 2
  # f: prediction function
  f_blocks: 2
  f_channels: 32

optimizer:
  learning_rate: 0.01
  momentum: 0.9
  weight_decay: 1.e-4
  nesterov: True
  rollout_steps: 5
  # Train the model when there are N newly generated states.
  frequency: 1000
  # How many states to store in the replay buffer.
  replay_buffer_size: 20000
  # How many times to reuse each state in the replay buffer.
  replay_buffer_reuse: 1
  # How many states to learn from per batch.
  batch_size: 200
  # save a checkpoint every N steps.
  checkpoint_freq: 10

mcts:
  # pUCT exploration constant
  c_puct: 1.25
  # MCTS simulation counts
  simulation_count: 400
  # discount factor of return
  discount_factor: 1.
  # nstep of the return
  nstep: 10
  dirichlet:
    # Dir(alpha)
    alpha: 0.03
    # (1 - epsilon) * policy + epsilon * Dir(a);
    epsilon: 0.25
